[
  {
    "objectID": "projects/project_1_mlp/project_1_mlp.html",
    "href": "projects/project_1_mlp/project_1_mlp.html",
    "title": "Predicting Song Release Year with MLP",
    "section": "",
    "text": "The goal of this project is to predict the release year of a song based on its attributes using a multi-layer perceptron neural network.\nFirst, we import the libraries we will need and load the dataset which is a subset of the Million Song Dataset. This dataset includes the year of the song and 90 attributes for each song.\n\n# Load required libraries\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(caret)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(corrplot)\n\n\ndf &lt;- read.csv('dataMS.csv')\n\nOn any data science project, we first perform an initial analysis of the dataset to understand its structure and characteristics.\nWe start by checking for any missing values in the dataset and then visualize the distribution of the target variable, which is the year of the song.\n\n# Check for missing values\nmissing_values &lt;- sum(is.na(df))\nprint(missing_values)\n\n[1] 0\n\n# Plot the distribution of the target variable\nggplot(df, aes(x = V1)) +\n  geom_histogram(binwidth = 1, fill = 'blue', color = 'black') +\n  xlab('Target Variable (V1)') +\n  ylab('Frequency') +\n  ggtitle('Distribution of the Target Variable')\n\n\n\n\nNext, we preprocess the data to prepare it for machine learning models. This involves splitting the data into features and target, scaling the features, and converting them to matrices.\nTherefore, here we split the dataset into training and testing sets, scale the features to ensure they have a mean of 0 and a standard deviation of 1, and manually scale the target variable (year) to ensure consistency.\n\n# Split data into features and target\nX &lt;- df[, -which(names(df) == \"V1\")]\ny &lt;- df$V1\n\n# Split data into training and test sets\nset.seed(123)\ntrain_index &lt;- createDataPartition(y, p = 0.4, list = FALSE)\ntrain_X &lt;- X[train_index, ]\ntrain_y &lt;- y[train_index]\ntest_X &lt;- X[-train_index, ]\ntest_y &lt;- y[-train_index]\n\n# Scale the features\nx_scaler &lt;- preProcess(train_X, method = c(\"center\", \"scale\"))\ntrain_X &lt;- predict(x_scaler, train_X)\ntest_X &lt;- predict(x_scaler, test_X)\n\n# Manually scale the target\ntrain_y_mean &lt;- mean(train_y)\ntrain_y_sd &lt;- sd(train_y)\ntrain_y &lt;- scale(train_y)\ntest_y &lt;- (test_y - train_y_mean) / train_y_sd\n\n# Convert to matrix\ntrain_X &lt;- as.matrix(train_X)\ntrain_y &lt;- as.matrix(train_y)\ntest_X &lt;- as.matrix(test_X)\ntest_y &lt;- as.matrix(test_y)\n\nWe then define and compile a multi-layer perceptron model using Keras. The model consists of three layers with ReLU activation functions, and it is compiled using Stochastic Gradient Descent (SGD) and Mean Squared Error (MSE) as the loss function.\n\n# Define the model\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 5, input_shape = ncol(train_X), activation = 'relu') %&gt;%\n  layer_dense(units = 5, activation = 'relu') %&gt;%\n  layer_dense(units = 1, activation = 'linear')\n\nsummary(model)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_2 (Dense)                    (None, 5)                       455         \n dense_1 (Dense)                    (None, 5)                       30          \n dense (Dense)                      (None, 1)                       6           \n================================================================================\nTotal params: 491 (1.92 KB)\nTrainable params: 491 (1.92 KB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\n# Compile the model\nmodel %&gt;% compile(\n  optimizer = optimizer_sgd(learning_rate = 0.01),\n  loss = 'mse'\n)\n\nTo effectively train the model, we divided the training data into two parts, one for actual training and the other for validation. The model was trained for 50 epochs, with each epoch processing data in batches of 32 samples.\n\n# Split the training data into training and validation sets\nset.seed(123)\ntrain_index2 &lt;- createDataPartition(train_y, p = 0.8, list = FALSE)\ntrain_X2 &lt;- train_X[train_index2, ]\ntrain_y2 &lt;- train_y[train_index2]\nval_X &lt;- train_X[-train_index2, ]\nval_y &lt;- train_y[-train_index2, ]\n\n# Train the model\nhistory &lt;- model %&gt;% fit(\n  train_X2, train_y2,\n  epochs = 50,\n  batch_size = 32,\n  validation_data = list(val_X, val_y),\n  verbose = 0 \n)\n\nWe evaluate the model’s performance on the test set and visualize the results. The Mean Squared Error (MSE) is calculated on the test set, and we plot the true vs. predicted values to assess the model’s accuracy.\n\n# Plot training and validation loss\nplot(history)\n\n\n\n# Predict on test data\npred_test_y &lt;- model %&gt;% predict(test_X)\n\n235/235 - 0s - 200ms/epoch - 851us/step\n\ntest_mse &lt;- mean((test_y - pred_test_y)^2)\n\ncat(\"Test MSE:\", round(test_mse, 3), \"\\n\")\n\nTest MSE: 0.504 \n\n# Plot true vs predicted values\nggplot(data = NULL, aes(x = test_y, y = pred_test_y)) +\n  geom_point() +\n  xlab(\"True Y\") +\n  ylab(\"Predicted Y\") +\n  ggtitle(\"True vs Predicted Values\")\n\n\n\n\nIn this project, I developed and evaluated a multi-layer perceptron model to predict the release year of songs based on their attributes from the Million Song Dataset. The model’s performance, evidenced by the low MSE and alignment of true and predicted values, showcases its ability to learn from the data and provide accurate predictions.\nThis project emphasizes the full machine learning pipeline, including data preprocessing, feature scaling, model building, training, and evaluation. It highlights key data science skills such as handling missing data, performing exploratory data analysis, implementing neural network models, and assessing their performance."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Welcome to the projects section of my portfolio. Here, you will find detailed case studies of some projects I have worked on. They demonstrate my data science knowledge, covering essential skills in areas such as data handling and preprocessing, exploratory data analysis, machine learning, clustering, dimensionality reduction, web scraping, data visualization and function creation and documentation.\nFeel free to explore each project to see detailed explanations, code snippets, visualizations, and results. If you have any questions about my projects, please don’t hesitate to contact me!"
  },
  {
    "objectID": "projects.html#about-this-site",
    "href": "projects.html#about-this-site",
    "title": "Projects",
    "section": "",
    "text": "Welcome to the projects section of my portfolio. Here, you will find detailed case studies of some projects I have worked on. They demonstrate my data science knowledge, covering essential skills in areas such as data handling and preprocessing, exploratory data analysis, machine learning, clustering, dimensionality reduction, web scraping, data visualization and function creation and documentation.\nFeel free to explore each project to see detailed explanations, code snippets, visualizations, and results. If you have any questions about my projects, please don’t hesitate to contact me!"
  },
  {
    "objectID": "projects/project_3_clustering/project_3_clustering.html",
    "href": "projects/project_3_clustering/project_3_clustering.html",
    "title": "Clustering Analysis",
    "section": "",
    "text": "In this project, we perform clustering analysis on the USArrests dataset. The primary goal is to identify patterns and groupings within the data using k-means and hierarchical clustering techniques.\nFirst, we load the necessary libraries for our analysis. These include libraries for data manipulation, visualization, and clustering. We then set a seed for reproducibility and load the dataset.\n\n# Load necessary libraries\nlibrary(ggplot2)    \nlibrary(GGally)     \nlibrary(cluster)    \nlibrary(dplyr)      \nlibrary(factoextra) \nlibrary(dendextend)\nlibrary(gridExtra)\nlibrary(dendextend)\n\n\n# Set seed for reproducibility\nset.seed(12345)\n\n# Load the dataset\ndf &lt;- read.csv(\"USArrests.csv\", row.names = 1)\n\nWe start with some exploratory data analysis (EDA) to understand the structure and relationships in the dataset. This includes checking the first few rows, creating pair plots, and visualizing the correlation matrix.\n\n# Check the first few rows of the dataframe\nhead(df)\n\n           Murder Assault UrbanPop Rape\nAlabama      13.2     236       58 21.2\nAlaska       10.0     263       48 44.5\nArizona       8.1     294       80 31.0\nArkansas      8.8     190       50 19.5\nCalifornia    9.0     276       91 40.6\nColorado      7.9     204       78 38.7\n\n# Pairplot\nggpairs(df)\n\n\n\n# Correlation matrix\ncor_matrix &lt;- cor(df)\ncorrplot::corrplot(cor_matrix, method = \"color\", type = \"upper\", tl.col = \"black\", tl.srt = 45)\n\n\n\n\nWe proceed with k-means clustering by first standardizing the data. To determine the optimal number of clusters, we calculate silhouette scores for a range of cluster numbers. The silhouette score measures how well each point lies within its cluster, with higher scores indicating better-defined clusters.\nFrom the silhouette score plot, we see that the highest score is achieved with 2 clusters, so we choose 2 as the optimal number for our k-means clustering.\nFinally, we perform k-means clustering with 2 clusters and visualize the results using PCA. PCA helps reduce the dimensionality of the data and allows us to visualize the clustering results in a 2D plot, making it easier to interpret the cluster structure.\n\n# Standardize the data\nscaled_df &lt;- scale(df)\n\n# Function to calculate silhouette score for given number of clusters\nnbc &lt;- function(n_clusters) {\n  km &lt;- kmeans(scaled_df, centers = n_clusters, nstart = 10)\n  silhouette_score &lt;- mean(silhouette(km$cluster, dist(scaled_df))[, 3])\n  return(silhouette_score)\n}\n\n# Calculate silhouette scores for a range of cluster numbers\nn_clusters &lt;- 2:15\nsilhouettes &lt;- sapply(n_clusters, nbc)\n\n# Plot silhouette scores\nplot(n_clusters, silhouettes, type = \"b\", xlab = \"n_clusters\", ylab = \"Silhouette Score\")\n\n\n\n# Perform k-means clustering with 2 clusters\nkm &lt;- kmeans(scaled_df, centers = 2, nstart = 10)\n\n# Perform PCA\npca &lt;- prcomp(scaled_df)\n\n# Create a dataframe with PCA results and cluster labels\npca_df &lt;- as.data.frame(pca$x)\npca_df$cluster_label &lt;- as.factor(km$cluster)\n\n# Pairplot with PCA results colored by cluster labels\nggpairs(pca_df, mapping = aes(color = cluster_label))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe finally perform hierarchical clustering using all combinations of complete, single, and average linkage methods with both Euclidean and Manhattan distances. This analysis reveals the significant impact of the linkage method choice on the clustering outcomes. Using “complete” or “average” linkage methods results in dendrograms that suggest the presence of two primary clusters of roughly equal size. In contrast, the “single” linkage method also indicates two clusters, but one of these clusters consists solely of the state of Alaska. The selection of the distance metric, whether Euclidean or Manhattan, has a relatively smaller influence on the final dendrograms for this dataset.\n\n# Define methods and metrics\nmethods &lt;- c(\"complete\", \"single\", \"average\")\nmetrics &lt;- c(\"euclidean\", \"manhattan\")\n\n# Create an empty list to store plots\nplots &lt;- list()\n\n# Perform hierarchical clustering with different methods and metrics\nfor (method in methods) {\n  for (metric in metrics) {\n    dist_matrix &lt;- dist(scaled_df, method = metric)\n    hdata &lt;- hclust(dist_matrix, method = method)\n    dend &lt;- as.dendrogram(hdata)\n    dend &lt;- set(dend, \"labels_cex\", 0.5)  \n    \n    # Color the branches\n    dend &lt;- color_branches(dend, k = 2)\n    \n    # Create the dendrogram plot \n    p &lt;- ggplot(as.ggdend(dend), theme_dendro = TRUE) + \n      ggtitle(paste(metric, \"distance and\", method, \"linkage\")) +\n      theme_minimal() + \n      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n            axis.text.y = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.title.y = element_blank())\n    \n    plots[[length(plots) + 1]] &lt;- p\n  }\n}\n\ngrid.arrange(grobs = plots, ncol = 2, top = \"Hierarchical Clustering Dendrograms\")\n\n\n\n\nThroughout the project, various data preprocessing, exploratory data analysis, and visualization techniques were employed to effectively communicate complex data insights. This analysis highlights the value of systematic approaches in data science for uncovering hidden structures and making informed decisions based on analytical results."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CID 02518535",
    "section": "",
    "text": "Hello! I’m [CID 02518535]. I completed my undergraduate studies in Mathematics and Statistics at the University of Cyprus and am currently pursuing my master’s degree in Statistics at Imperial College London. I created this portfolio to demonstrate my growing knowledge and skills in data science.\nI invite you to explore my detailed projects where I showcase the methodologies and techniques I have learned. Visit the Projects Page to learn more. I’m always excited to connect with fellow data enthusiasts, professionals, and potential collaborators. Feel free to reach out to me using the contact buttons below.\nThank you for visiting my portfolio!"
  },
  {
    "objectID": "projects/project_2_web_scraping/project_2_web_scraping.html",
    "href": "projects/project_2_web_scraping/project_2_web_scraping.html",
    "title": "Data Web Scraping",
    "section": "",
    "text": "During academic studies, data is typically provided in user friendly formats. However, in real-world scenarios, it is often necessary to retrieve data embedded within websites. Obtaining such data is a fundamental aspect of Data Science. This project demonstrates how to use web scraping techniques to acquire such data using the rvest package in R. We present two simple examples: scraping book data from a website and extracting the most popular baby names from a government statistics page.\n1st Example: Scraping Book Data\nThe goal of this example is to demonstrate how to scrape book information from the “Books to Scrape” website. It features various book categories (each with randomly assigned prices and ratings). This site is specifically designed for web scraping practice and it is a safe playground for learning and practicing web scraping techniques!\nFor this project, we will use three R libraries essential for web scraping, data manipulation, and visualization. I want to mention rvest specifically because it is the most important for this project. This library is specifically designed for web scraping, it simplifies the process of downloading web pages and extracting data from them.\n\n# Loading Required Libraries\n# rvest for web scraping, ggplot2 for visualization, and dplyr for data manipulation\n\nlibrary(rvest)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nWe start by defining the URL and using the read_html to parse the HTML content. This allows us to access and manipulate the data embedded in the webpage.\n\n# Define the URL for Books to Scrape\nurl &lt;- \"http://books.toscrape.com/\"\n\n# Send GET request and parse HTML\npage &lt;- read_html(url)\n\nThe next step is to use CSS selectors and extract the book titles, prices, and ratings from the HTML content. This basically shows the ability to navigate and manipulate HTML structures to obtain the desired data.\n\n# Extract book titles\ntitles &lt;- page %&gt;% html_nodes(\".product_pod h3 a\") %&gt;% html_attr(\"title\")\n\n# Extract book prices\nprices &lt;- page %&gt;% html_nodes(\".product_price .price_color\") %&gt;% html_text()\n\n# Extract book ratings\nratings &lt;- page %&gt;% html_nodes(\".star-rating\") %&gt;% html_attr(\"class\") %&gt;% gsub(\"star-rating \", \"\", .)\n\nWe then combine the extracted data into a data frame and clean the prices by removing the currency symbol and converting them to numeric values, to ensure that the data is ready for analysis.\n\n# Combine data into a data frame\nbooks &lt;- data.frame(Title = titles, Price = prices, Rating = ratings, stringsAsFactors = FALSE)\n\n# Convert prices to numeric values (removing '£')\nbooks$Price &lt;- gsub(\"£\", \"\", books$Price) %&gt;% as.numeric()\n\n# Print cleaned data\nprint(head(books))\n\n                                  Title Price Rating\n1                  A Light in the Attic 51.77  Three\n2                    Tipping the Velvet 53.74    One\n3                            Soumission 50.10    One\n4                         Sharp Objects 47.82   Four\n5 Sapiens: A Brief History of Humankind 54.23   Five\n6                       The Requiem Red 22.65    One\n\n\nFinally, we create a histogram to visualize the distribution of the book prices.\n\n# Plot the distribution of book prices\nggplot(books, aes(x = Price)) +\n  geom_histogram(binwidth = 5, fill = \"blue\", color = \"black\") +\n  ggtitle(\"Distribution of Book Prices\") +\n  xlab(\"Price (in GBP)\") +\n  ylab(\"Frequency\") +\n  theme_minimal()\n\n\n\n\n2nd Example: Scraping Baby Names Data\nThis is another example, for which we extract baby names data from the ONS website. This site provides data on various societal metrics, including popular baby names. The data is presented in tables.\nAgain, we begin by fetching the content of the webpage. We focus on the table containing the baby names data.\n\nhtml &lt;- read_html(\"https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/bulletins/babynamesenglandandwales/2021\")\n\nnames&lt;-html %&gt;% \n  html_element(\"table\") %&gt;%\n  html_table()\n\nnames\n\n# A tibble: 10 × 7\n    Rank `Boys name` Count `Change in rank since 2020` `Girls name` Count\n   &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;                       &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;\n 1     1 Noah        4,525                           3 Olivia       3,649\n 2     2 Oliver      4,167                          -1 Amelia       3,164\n 3     3 George      4,141                          -1 Isla         2,683\n 4     4 Arthur      3,766                          -1 Ava          2,576\n 5     5 Muhammad    3,722                           0 Ivy          2,245\n 6     6 Leo         3,465                           0 Freya        2,187\n 7     7 Harry       3,089                           1 Lily         2,182\n 8     8 Oscar       3,071                          -1 Florence     2,180\n 9     9 Archie      2,928                           0 Mia          2,168\n10    10 Henry       2,912                           1 Willow       2,067\n# ℹ 1 more variable: `Change in rank since 2020` &lt;int&gt;\n\n\nWe clean the data by removing non numeric characters from the count columns and separate the data into two data frames, one for boys and one for girls names.\n\n# Assign new column names to avoid duplicates\ncolnames(names) &lt;- c(\"Rank\", \"BoysName\", \"BoysCount\", \"BoysChange\", \"GirlsName\", \"GirlsCount\", \"GirlsChange\")\n\n# Clean the Count columns to remove non-numeric characters\nnames &lt;- names %&gt;%\n  mutate(BoysCount = as.numeric(gsub(\",\", \"\", BoysCount)),\n         GirlsCount = as.numeric(gsub(\",\", \"\", GirlsCount)))\n\n# Separate boys' and girls' names into two data frames\nboys_names &lt;- names %&gt;%\n  select(Rank, BoysName, BoysCount, BoysChange) %&gt;%\n  rename(Name = BoysName, Count = BoysCount, Change = BoysChange)\n\ngirls_names &lt;- names %&gt;%\n  select(Rank, GirlsName, GirlsCount, GirlsChange) %&gt;%\n  rename(Name = GirlsName, Count = GirlsCount, Change = GirlsChange)\n\nLastly, we create bar plots to visualize the top 10 boys’ and girls’ names.\n\n# Top 10 Boys Names\nggplot(boys_names, aes(x = reorder(Name, -Count), y = Count)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  labs(title = \"Top 10 Boys Names in 2021\",\n       x = \"Boys Name\",\n       y = \"Count\")\n\n\n\n# Top 10 Girls Names\nggplot(girls_names, aes(x = reorder(Name, -Count), y = Count)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  labs(title = \"Top 10 Girls Names in 2021\",\n       x = \"Girls Name\",\n       y = \"Count\")\n\n\n\n\nWe demonstrated essential data science skills through web scraping. Using the rvest package in R, we successfully extracted and cleaned data from websites, and visualized the results to gain some insights."
  },
  {
    "objectID": "projects/project_1_mlp/project_1_pol_regression.html",
    "href": "projects/project_1_mlp/project_1_pol_regression.html",
    "title": "Polynomial Regression Analysis",
    "section": "",
    "text": "generate_data &lt;- function() {\n  # Standard deviation of the noise to be added\n  noise_std &lt;- 100\n  \n  # Set up the input variable, 100 points between -5 and 5\n  x &lt;- seq(-5, 5, length.out = 100)\n  \n  # Calculate the true function and add some noise\n  y &lt;- 5 * x^3 - x^2 + x + rnorm(length(x), mean = 0, sd = noise_std)\n  \n  # Combine x and y into a data frame and return\n  data &lt;- data.frame(x = x, y = y)\n  \n  return(data)\n}\n\ndata &lt;- generate_data()\nplot(data$x, data$y, main = \"Generated Data\")\n\n\n\n\n\nget_mse &lt;- function(x, y, poly_order) {\n  # Construct design matrix of given order\n  X &lt;- as.matrix(cbind(1, poly(x, poly_order, raw = TRUE)))\n  \n  # Solve for coefficients\n  coefs &lt;- solve(t(X) %*% X, t(X) %*% y)\n  pred_y &lt;- X %*% coefs\n  \n  # Calculate MSE\n  mse &lt;- mean((pred_y - y)^2)\n  \n  return(mse)\n}\n\n\norders &lt;- 1:8\nmse_values &lt;- sapply(orders, function(p) get_mse(data$x, data$y, p))\nplot(orders, mse_values, type = \"b\", main = \"MSE vs Polynomial Order\", xlab = \"Order\", ylab = \"MSE\")\n\n\n\n\n\nset.seed(3)\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nget_mse_randomsplit &lt;- function(x, y, poly_order) {\n  # Construct design matrix of given order\n  X &lt;- as.matrix(cbind(1, poly(x, poly_order, raw = TRUE)))\n  \n  # Split data into training and testing sets\n  train_indices &lt;- createDataPartition(y, p = 0.7, list = FALSE)\n  X_train &lt;- X[train_indices, ]\n  X_test &lt;- X[-train_indices, ]\n  y_train &lt;- y[train_indices]\n  y_test &lt;- y[-train_indices]\n  \n  # Solve for coefficients\n  coefs &lt;- solve(t(X_train) %*% X_train, t(X_train) %*% y_train)\n  pred_y_test &lt;- X_test %*% coefs\n  pred_y_train &lt;- X_train %*% coefs\n  \n  # Calculate MSE for test and train sets\n  mse_test &lt;- mean((pred_y_test - y_test)^2)\n  mse_train &lt;- mean((pred_y_train - y_train)^2)\n  \n  return(c(mse_test, mse_train))\n}\n\n\nmse_randomsplit &lt;- sapply(orders, function(p) get_mse_randomsplit(data$x, data$y, p))\nplot(orders, mse_randomsplit[1, ], type = \"b\", col = \"red\", ylim = range(mse_randomsplit), ylab = \"MSE\", xlab = \"Order\", main = \"MSE for Train and Test\")\nlines(orders, mse_randomsplit[2, ], type = \"b\", col = \"blue\")\nlegend(\"topright\", legend = c(\"Test\", \"Train\"), col = c(\"red\", \"blue\"), lty = 1)\n\n\n\n\n\nget_LOOCV &lt;- function(x, y, poly_order) {\n  # Construct design matrix of given order\n  X &lt;- as.matrix(cbind(1, poly(x, poly_order, raw = TRUE)))\n  \n  # Initialize vector for CV errors\n  CV &lt;- numeric(length(x))\n  \n  for (i in 1:length(x)) {\n    X_train &lt;- X[-i, , drop = FALSE]\n    X_test &lt;- X[i, , drop = FALSE]\n    y_train &lt;- y[-i]\n    y_test &lt;- y[i]\n    \n    # Solve for coefficients\n    coefs &lt;- solve(t(X_train) %*% X_train, t(X_train) %*% y_train)\n    pred_y &lt;- X_test %*% coefs\n    \n    # Calculate MSE for the left-out observation\n    CV[i] &lt;- mean((pred_y - y_test)^2)\n  }\n  \n  cv_mean &lt;- mean(CV)\n  cv_sd &lt;- sd(CV)\n  \n  return(c(cv_mean, cv_sd))\n}\n\n\nloocv_results &lt;- sapply(orders, function(p) get_LOOCV(data$x, data$y, p))\nplot(orders, loocv_results[1, ], type = \"b\", ylab = \"LOOCV Mean MSE\", xlab = \"Order\", main = \"LOOCV Mean MSE vs Polynomial Order\")"
  },
  {
    "objectID": "projects/project_1_pol_regression/project_1_pol_regression.html",
    "href": "projects/project_1_pol_regression/project_1_pol_regression.html",
    "title": "Polynomial Regression Analysis",
    "section": "",
    "text": "In this project, we explore the concepts of overfitting and model selection using polynomial regression on generated data. The goal is to understand how polynomial order affects model performance and to use techniques such as train-test split and Leave-One-Out Cross-Validation to select the appropriate model. Through this analysis, we demonstrate key data science skills, including data generation, function creation and documentation, model evaluation, and visualization.\nFirst, we load the necessary libraries.\n\nlibrary(caret)\nlibrary(ggplot2)\n\nNext, we generate synthetic data with a known underlying polynomial relationship and add some noise to it. The ability to create synthetic data is an essential skill in data science.\n\n#' Generate synthetic data with noise\n#'\n#' @return A data frame containing the generated data with x and y columns\ngenerate_data &lt;- function() {\n  # Standard deviation of the noise to be added\n  noise_std &lt;- 100\n  \n  # Set up the input variable, 100 points between -5 and 5\n  x &lt;- seq(-5, 5, length.out = 100)\n  \n  # Calculate the true function and add some noise\n  y &lt;- 5 * x^3 - x^2 + x + rnorm(length(x), mean = 0, sd = noise_std)\n  \n  # Combine x and y into a data frame and return\n  data &lt;- data.frame(x = x, y = y)\n  \n  return(data)\n}\n\ndata &lt;- generate_data()\nplot(data$x, data$y, main = \"Generated Data\")\n\n\n\n\nWe create a function to calculate the MSE for a given polynomial order and document it using Roxygen2-style comments (as we do with all the functions we create).\n\n#' Calculate Mean Squared Error (MSE) for a polynomial regression model\n#'\n#' @param x A numeric vector of predictor values\n#' @param y A numeric vector of response values\n#' @param poly_order An integer representing the polynomial order\n#'\n#' @return The MSE of the polynomial regression model\nget_mse &lt;- function(x, y, poly_order) {\n  # Construct design matrix of given order\n  X &lt;- as.matrix(cbind(1, poly(x, poly_order, raw = TRUE)))\n  \n  # Solve for coefficients\n  coefs &lt;- solve(t(X) %*% X, t(X) %*% y)\n  pred_y &lt;- X %*% coefs\n  \n  # Calculate MSE\n  mse &lt;- mean((pred_y - y)^2)\n  \n  return(mse)\n}\n\nWe calculate the MSE for polynomial orders from 1 to 8 and plot the results.\n\norders &lt;- 1:8\nmse_values &lt;- sapply(orders, function(p) get_mse(data$x, data$y, p))\nplot(orders, mse_values, type = \"b\", main = \"MSE vs Polynomial Order\", xlab = \"Order\", ylab = \"MSE\")\n\n\n\n\nWe use a random split to divide the data into training and testing sets, then calculate the MSE for both sets.\n\nset.seed(3)\n\n#' Calculate MSE for train and test sets using random split\n#'\n#' @param x A numeric vector of predictor values\n#' @param y A numeric vector of response values\n#' @param poly_order An integer representing the polynomial order\n#'\n#' @return A numeric vector containing the test and train MSE\nget_mse_randomsplit &lt;- function(x, y, poly_order) {\n  # Construct design matrix of given order\n  X &lt;- as.matrix(cbind(1, poly(x, poly_order, raw = TRUE)))\n  \n  # Split data into training and testing sets\n  train_indices &lt;- createDataPartition(y, p = 0.7, list = FALSE)\n  X_train &lt;- X[train_indices, ]\n  X_test &lt;- X[-train_indices, ]\n  y_train &lt;- y[train_indices]\n  y_test &lt;- y[-train_indices]\n  \n  # Solve for coefficients\n  coefs &lt;- solve(t(X_train) %*% X_train, t(X_train) %*% y_train)\n  pred_y_test &lt;- X_test %*% coefs\n  pred_y_train &lt;- X_train %*% coefs\n  \n  # Calculate MSE for test and train sets\n  mse_test &lt;- mean((pred_y_test - y_test)^2)\n  mse_train &lt;- mean((pred_y_train - y_train)^2)\n  \n  return(c(mse_test, mse_train))\n}\n\nWe plot the MSE for both training and testing sets across different polynomial orders.\n\nmse_randomsplit &lt;- sapply(orders, function(p) get_mse_randomsplit(data$x, data$y, p))\nplot(orders, mse_randomsplit[1, ], type = \"b\", col = \"red\", ylim = range(mse_randomsplit), ylab = \"MSE\", xlab = \"Order\", main = \"MSE for Train and Test\")\nlines(orders, mse_randomsplit[2, ], type = \"b\", col = \"blue\")\nlegend(\"topright\", legend = c(\"Test\", \"Train\"), col = c(\"red\", \"blue\"), lty = 1)\n\n\n\n\nWe implement Leave-One-Out Cross-Validation to evaluate the model.\n\n#' Perform Leave-One-Out Cross-Validation (LOOCV) for a polynomial regression model\n#'\n#' @param x A numeric vector of predictor values\n#' @param y A numeric vector of response values\n#' @param poly_order An integer representing the polynomial order\n#'\n#' @return A numeric vector containing the mean and standard deviation of LOOCV MSE\nget_LOOCV &lt;- function(x, y, poly_order) {\n  # Construct design matrix of given order\n  X &lt;- as.matrix(cbind(1, poly(x, poly_order, raw = TRUE)))\n  \n  # Initialize vector for CV errors\n  CV &lt;- numeric(length(x))\n  \n  for (i in 1:length(x)) {\n    X_train &lt;- X[-i, , drop = FALSE]\n    X_test &lt;- X[i, , drop = FALSE]\n    y_train &lt;- y[-i]\n    y_test &lt;- y[i]\n    \n    # Solve for coefficients\n    coefs &lt;- solve(t(X_train) %*% X_train, t(X_train) %*% y_train)\n    pred_y &lt;- X_test %*% coefs\n    \n    # Calculate MSE for the left-out observation\n    CV[i] &lt;- mean((pred_y - y_test)^2)\n  }\n  \n  cv_mean &lt;- mean(CV)\n  cv_sd &lt;- sd(CV)\n  \n  return(c(cv_mean, cv_sd))\n}\n\nWe calculate and plot the LOOCV results for different polynomial orders.\n\nloocv_results &lt;- sapply(orders, function(p) get_LOOCV(data$x, data$y, p))\nplot(orders, loocv_results[1, ], type = \"b\", ylab = \"LOOCV Mean MSE\", xlab = \"Order\", main = \"LOOCV Mean MSE vs Polynomial Order\")\n\n\n\n\nBy generating synthetic data and applying polynomial regression, we explored the impact of model complexity on performance. Using techniques such as train-test split and LOOCV, we evaluated our models and highlighted the importance of proper model selection to prevent overfitting. Additionally, we showed that creating functions and documenting them using Roxygen2-style comments is a crucial skill for maintaining clear and understandable code, which is essential for collaborative data science work and reproducibility."
  }
]